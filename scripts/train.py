#!/usr/bin/env python3
"""Train the wall/static/opponent ensemble using linear and logistic regression."""

from __future__ import annotations

import argparse
import datetime
import json
import random
import sys
from pathlib import Path
from typing import Any, Dict, List, Mapping, Sequence, Tuple

ROOT_DIR = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(ROOT_DIR))

from models.LinearRegression import classify_frame
from models.LogisticRegression import LogisticRegression
from scripts.data_utils import (
    build_dataset,
    load_feature_index,
    read_scan_csv,
    split_frames,
)
from scripts.eval_utils import evaluate_split

DEFAULT_CLASS_NAMES = ["wall", "static", "opponent", "free"]


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description="Train the wall/opponent classifier with linear+logistic ensembles."
    )
    parser.add_argument(
        "--csv",
        "-c",
        type=Path,
        required=True,
        help="CSV file containing the LiDAR scans.",
    )
    parser.add_argument(
        "--feature-index",
        "-j",
        type=Path,
        required=True,
        help="JSON map generated by feature_vector_index_generator.py.",
    )
    parser.add_argument(
        "--threshold",
        "-t",
        type=float,
        default=0.25,
        help="Max distance from a fitted line to consider a scan as wall.",
    )
    parser.add_argument(
        "--split-ratio",
        type=float,
        default=0.8,
        help="Train/test split ratio (default 0.8).",
    )
    parser.add_argument(
        "--epochs",
        type=int,
        default=40,
        help="Number of epochs for logistic regression training.",
    )
    parser.add_argument(
        "--learning-rate",
        type=float,
        default=0.01,
        help="Learning rate for the logistic optimizer.",
    )
    parser.add_argument(
        "--seed",
        type=int,
        default=42,
        help="Random seed for deterministic train/test splitting.",
    )
    parser.add_argument(
        "--output",
        "-o",
        type=Path,
        default=Path("checkpoints/linear_wall_checkpoint.json"),
        help="Checkpoint JSON that records the trained ensemble.",
    )
    return parser.parse_args()


def main() -> None:
    args = parse_args()
    feature_map, feature_payload = load_feature_index(args.feature_index)
    vector_length = int(feature_payload.get("vector_length", 360))
    frames, frame_vectors, entries = read_scan_csv(args.csv, feature_map)
    if not entries:
        raise SystemExit("No valid scan rows found.")
    rng = random.Random(args.seed)
    frame_indices = sorted(frames.keys())
    train_frames, test_frames = split_frames(frame_indices, args.split_ratio, rng)
    train_features, train_labels, train_meta = build_dataset(
        entries, frame_vectors, vector_length, train_frames
    )
    if not train_features:
        raise SystemExit("Train split contains no samples.")
    logistic = LogisticRegression(
        num_features=vector_length,
        num_classes=len(DEFAULT_CLASS_NAMES),
        learning_rate=args.learning_rate,
    )
    print(f"Training logistic regression on {len(train_features)} samples...")
    history = logistic.fit(
        train_features, train_labels, epochs=args.epochs, verbose=True
    )
    linear_map: Dict[Tuple[int, int], Mapping[str, float]] = {}
    frame_results: Dict[str, Mapping[str, Any]] = {}
    for frame_index in sorted(frames.keys()):
        classification = classify_frame(frames[frame_index], threshold=args.threshold)
        frame_results[str(frame_index)] = classification
        for point in classification["point_info"]:
            linear_map[(frame_index, int(point["scan_index"]))] = point
    train_metrics = evaluate_split(
        "train",
        train_features,
        train_labels,
        train_meta,
        logistic,
        linear_map,
        args.threshold,
    )
    if train_metrics["logistic_accuracy"] is not None:
        print(f"Train logistic accuracy: {train_metrics['logistic_accuracy']:.3f}")
    if train_metrics["ensemble_accuracy"] is not None:
        print(f"Train ensemble accuracy: {train_metrics['ensemble_accuracy']:.3f}")

    checkpoint = {
        "csv_file": str(args.csv.name),
        "feature_index": str(args.feature_index.name),
        "threshold": args.threshold,
        "split_ratio": args.split_ratio,
        "seed": args.seed,
        "train_frames": train_frames,
        "test_frames": test_frames,
        "logistic": logistic.to_dict(),
        "logistic_loss_history": history,
        "frames": frame_results,
        "train_metrics": train_metrics,
    }
    args.output.parent.mkdir(parents=True, exist_ok=True)
    with args.output.open("w", encoding="utf-8") as handle:
        json.dump(checkpoint, handle, ensure_ascii=False, indent=2)
    diag_dir = Path("linear_regression_metadata")
    diag_dir.mkdir(parents=True, exist_ok=True)
    diag_payload = {
        "created_at": datetime.datetime.now().isoformat(),
        "frames": {
            idx: {
                "wall_ratio": data["wall_ratio"],
                "line_models": data["line_models"],
                "line_support": [model["support_total"] for model in data["line_models"]],
            }
            for idx, data in frame_results.items()
        },
    }
    diag_file = diag_dir / f"linear_regression_metadata_{int(datetime.datetime.now().timestamp())}.json"
    with diag_file.open("w", encoding="utf-8") as diag_handle:
        json.dump(diag_payload, diag_handle, ensure_ascii=False, indent=2)
    print(f"Checkpoint saved to {args.output}")


if __name__ == "__main__":
    main()
